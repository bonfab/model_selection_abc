\documentclass[a4paper, 11pt]{article}

\usepackage{geometry}
\geometry{a4paper,left=30mm,right=30mm, top=35mm, bottom=30mm}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{csquotes}

\usepackage{lastpage} % Seitenzahlen

\pagestyle{fancy}
\usepackage{amsmath}
\usepackage{tabularx} %schöne tabellen
\parindent0pt %einrücken verhindern
\renewcommand{\familydefault}{\sfdefault} % use sans fonts


\cfoot{\thepage  \ / \pageref{LastPage}}

\usepackage[
backend=biber,
%style=ieee,
citestyle=authoryear
]{biblatex}

% % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % %
\newcommand{\thesis}{Bachelor Thesis}
\newcommand{\topic}{Inferring the Population Quantity of Multilocus Genotype Data}
\newcommand{\modul}{Bachelor Thesis}
\newcommand{\supervisor}{Supervisors: Manfred Opper, \\ Olivier Fran\c{c}ios, \\ Michael Blum}
\newcommand{\datum}{\today}
% % % % % % % % % % % % % % % % % % % % % % % %
% % % % % % % % % % % % % % % % % % % % % % % %


\addbibresource{sources.bib}

\begin{document} 

%%% Kopfzeile linker Bereich
%      gerade Seite   ungerade Seite
\lhead[ \leftmark   ]{\textbf{\modul}}
%%% Kopfzeile mittlerer Bereich
%      gerade Seite   ungerade Seite
\chead[\rightmark   ]{\rightmark{}}
%%% Kopfzeile linker Bereich
%      gerade Seite             ungerade Seite
\rhead[\textbf{text}]{\supervisor}


	%-- Deckblatt --						      
  \title{\textbf{\thesis\\[0.5cm] \Large{\topic} \\[1.5cm]}
	\author{%\tutorium\\ \\
		Fabian Bergmann, 372918\\ \\ \\ \\ \\ \\ \\ \\ 
		Pages: \pageref{LastPage} \\ \\ \\ \\
		}
	\normalsize{\supervisor}} %Thema ändern	
	\date{Submission Date: \datum} %Datum ändern
	\maketitle
	\newpage
	
\renewcommand{\labelenumi}{\alph{enumi})}
\renewcommand{\labelenumii}{(\roman{enumii})}
\renewcommand{\labelenumiii}{\arabic{enumiii}.}
%\renewcommand{\labelenumii}{\textbf{-}}	
%-- Eigentlicher Text --

\section{Generating Data}

\subsection{Biological Background}

\subsubsection{Key words}

\begin{itemize}
\item \textbf{Chromosome:} A DNA molecule that encodes genetic information.

\item \textbf{Gene:} A DNA (or RNA) sequence that specifies the structure of a particular functional molecule.

\item \textbf{Locus:} A particular position on the chromosome, like the position of a specific gene.

\item \textbf{Allele:} A variant form of a given gene. Different alleles can lead to distinct phenotypic traits.

\end{itemize}

\subsection{Admixture}
The subsequent admixture model, follows a model proposed by \cite{pritchard2000inference}. 

\newpage

\section{Theory}

\subsection{ABC}

To infer the number of populations $K$ expressed in a given dataset $X$ the conditional probability $P(K | X)$ with respect to $K$ is maximized. Since the large dimensionalities of the used datasets pose substantial computational difficulties, the datasets are summarized  in an effective manner, such that the approximation $P(K|X) \approx P(K | sum(X))$ is sufficient for the intended inference. Bayes' theorem then yields

\begin{align*}
P(K | sum(X)) = \frac{P(sum(X) | K) P(K)}{P(sum(X))}
\end{align*}

The calculation of the likelihood $P(sum(X) | K)$ however is intractable because \textbf{???}
To circumvent this problem the likelihood is implicitly calculated by employing a supervised learning method to estimate the posterior, such as a neural network or boosting decision trees. These methods are trained by trying to link summary statistics of datasets to the corresponding values for the number of populations. The prior $P(K)$ 
can be implicitly adjusted by changing the proportions of $K$ in the training data. Gradient boosting with decision trees is chosen in this case, for it has demonstrated good results for various classification problems citation needed. \textbf{??? Furthermore, some inuitive reasoning exists, as explained later on, for the use of decision trees in this particular case.}

\subsection{Choosing the Summary Statistics}

The choice of an adequate summary statistics is essentiell to obtain significant results. On the one hand a summary of the data is necessary to handle its large dimensionality, on the other hand the manner of summarisation has to be chosen carefully to sustain the vital information necessary for the inference, because each summarisation consequently forfeits some of the principal information. 

\begin{align*}
h(x) & = - \int_{-\infty}^{\infty} N(x|\mu, \Sigma)ln(N(x|\mu, \Sigma))dx \\
& = E[ln(N(x|\mu, \Sigma)] \\
& = E[ln(det(2\pi\Sigma)^{-\frac{1}{2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)})] \\
& = \frac{1}{2} ln(det(2\pi\Sigma)) + \frac{1}{2} E[(x-\mu)^T \Sigma^{-1}(x-\mu)]\\
& = \frac{1}{2} ln(det(2\pi\Sigma)) + \frac{1}{2} E[trace(\Sigma^{-1}(x-\mu)^T (x-\mu))]\\
& = \frac{1}{2} ln(det(2\pi\Sigma)) + \frac{1}{2} E[trace(I)]\\
& = \frac{1}{2} ln(det(2\pi e \Sigma))
\end{align*}

\subsubsection{PCA}

\subsection{RMT}

Let $\mathbf{A} = \frac{1}{n}\mathbf{X}\mathbf{X}^{T}$ be the empirical covariance matrix of $\mathbf{X}$ with $\mathbf{X}$ being an $m \times n$ matrix. $\lambda_{1},\lambda_{2}, \dots, \lambda_{m}$ are the corresponding eigenvalues of $\mathbf{A}$. The empirical spectral distribution (\textbf{ESD}) for  $\mathbf{M}$ is then given by:

\begin{align*}
F^{M}(x) = \frac{1}{m} \mid \{\lambda_{i} \leq x \mid i \leq m\} \mid
\end{align*}

Whereby $\mid \cdot \mid$ denotes the size of a set.
\\
\\
By assuming a theoretical setting in which $m, n \rightarrow \infty$ while $y = \frac{m}{n} \rightarrow (0, \infty)$ the Marchenko-Pastur Law extends the ESD to the continuous case.

Under the assumption that the entries of $\mathbf{X}$ are random variables iid distributed with mean $0$, it states that the probability density of the eigenvalues is given by:

\begin{align*}
p^{M}(x) = \frac{1}{2\pi xy \sigma^{2}}  \sqrt[]{(\rho_{+} - x)(x - \rho_{-})}
\end{align*}

where $\rho_{\pm} = \sigma^{2}(1 \pm \sqrt[]{y})^{2}$ and $\sigma^2$ is the variance of the random variables.
\\
\textbf{Insert Plot of distribution}
\\


\newpage

\section{Gradient Boosting}

\printbibliography

\end{document}
